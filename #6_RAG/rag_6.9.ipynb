{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.9 Map Reduce LCEL Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 LCEL을 이용해서 Map Reduce chain을 직접 구현해볼 것이다.\n",
    "\n",
    "일단은 단순화된 버전을 만들 것이다. 복잡한 기능의 추가는 나중에 얼마든지 가능하기 때문이다.<br>\n",
    "token을 세어주거나 prompt가 context window에 적합한지 확인하는 그런거 말이다. 일단 단순하게 하나 만들어보면, 어떻게 동작하는지 이해할 수 있고, 직접 만들 수 있다는 확신도 갖게 된다.<br><br>\n",
    "\n",
    "Map Reduce가 어떤식으로 작동하는지에대해 알아보자.<br>\n",
    "일단은 질문과 관련된 document의 list를 얻어야 한다. 그 다음으로는 list내부의 모든 document들을 위한 prompt를 만들어 줄 것이다. 그 prompt는 LLM에게 전달할건데, 기본적인 내용은 다음과 같다.\n",
    "\n",
    "    '이 document를 읽고, 사용자의 질문에 답변하기에 적절한 정보가 있는지 확인하고, 있다면 추출해 주세요.'\n",
    "\n",
    "이를 전달받은 LLM은 응답(response)을 출력할 것이다. 그리고 LLM으로부터 받은 response들을 취합해 하나의 document를 만들어낼 것이다. 즉, 하나의 document에 합친다는 말이다.<br>\n",
    "그렇게 만들어진 단 하나의 최종 document가, LLM을 위한 prompt로 전달될 것이다. 전달될때의 prompt내용은 다음과 같을 것이다.\n",
    "\n",
    "    '이것은 질문과 관련이 있는 정보들 입니다. 이를 사용하여 대답해주세요'\n",
    "\n",
    "이렇게 되면 마침내 처음의 질문에 대한 답변이 생성될 것이다.\n",
    "\n",
    "<br>\n",
    "\n",
    "그렇다면 현재 이 방식과 stuff중 어떤 상황에서 어느것을 사용하는 것이 더 효율적일까? 정답은 바로, 우리가 원하는 prompt의 크기와 검색할 document의 수에 따라 달라진다. 만약 retriever가 검색 결과로 천 개이상의 document를 반환한다면, stuff는 사용할 수 없다. 왜냐하면 stuff의 prompt에 그 document들을 모드 넣을 수 없기 때문이다. <br>\n",
    "바로 이런 상황이 Map Reduce방식이 빛을 발하는 순간이다.\n",
    "\n",
    "나중에 우리가 회의 GPT를 만들때에도 이런 형식의 코드로 구현할 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The name of Oldwyn's mechanical doll is Vesper.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"../files/chapter_one.docx\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache\")\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "     embeddings,\n",
    "     cache_dir,\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        Use the following portion of a long document to see if any of the text is relevant to anser the question\n",
    "        Return any relevant text verbatim.\n",
    "        ------\n",
    "        {context}\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs['documents']\n",
    "    question = inputs['question']\n",
    "    results = []\n",
    "    for document in documents:\n",
    "        result = map_doc_chain.invoke({\n",
    "            \"context\": document.page_content,\n",
    "            \"question\": question\n",
    "        }).content\n",
    "        results.append(result)\n",
    "    results = \"\\n\\n\".join(results)\n",
    "    return results\n",
    "\n",
    "map_chain = {\"documents\": retriever, \"question\": RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\", \n",
    "        \"\"\"\n",
    "        Given the following extracted parts of a long docement and a question, create a final anwer.\n",
    "        If you don't know the answer, just say that you don't know.\n",
    "        Don't try to make up an answer.\n",
    "        ------\n",
    "        {context}\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"What is the name of Oldwyn’s mechanical doll?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 문서 내 관련 내용 추출을 위한 프롬프트 생성 (라인 37 ~ 48)\n",
    "\n",
    "- **라인 37:**  \n",
    "  ```python\n",
    "  map_doc_prompt = ChatPromptTemplate.from_messages([\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - `ChatPromptTemplate.from_messages` 메서드를 사용하여, LLM(대형 언어 모델)에게 전달할 메시지 템플릿을 생성합니다.  \n",
    "    - 이 템플릿은 이후 각 문서 조각의 내용(context)와 질문(question)을 LLM에 전달할 때 사용됩니다.\n",
    "\n",
    "- **라인 38 ~ 46:**  \n",
    "  ```python\n",
    "      (\n",
    "          \"system\",\n",
    "          \"\"\"\n",
    "          Use the following portion of a long document to see if any of the text is relevant to anser the question\n",
    "          Return any relevant text verbatim.\n",
    "          ------\n",
    "          {context}\n",
    "          \"\"\"\n",
    "      ),\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - 첫 번째 메시지는 `\"system\"` 역할로, LLM에게 시스템 지시사항을 전달합니다.\n",
    "    - 멀티라인 문자열 안에 다음의 내용이 포함되어 있습니다:  \n",
    "      - **지시사항:**  \n",
    "        - “Use the following portion of a long document to see if any of the text is relevant to anser the question”  \n",
    "          → 주어진 문서 일부에서 질문에 관련된 내용을 찾으라는 명령입니다.\n",
    "        - “Return any relevant text verbatim.”  \n",
    "          → 관련 있는 텍스트가 있다면 원문 그대로 반환하라는 지시입니다.\n",
    "        - “------” 구분선과 함께 `{context}` 플레이스홀더가 있어, 이후 실제 문서 내용이 이 자리에 삽입됩니다.\n",
    "\n",
    "- **라인 47:**  \n",
    "  ```python\n",
    "      (\"human\", \"{question}\")\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - 두 번째 메시지는 `\"human\"` 역할로, 실제 사용자의 질문이 들어갈 부분을 `{question}` 플레이스홀더로 지정합니다.\n",
    "    - 이 구조는 prompt에 “시스템 지시사항”과 “사용자 질문” 두 부분을 모두 포함하게 됩니다.\n",
    "\n",
    "- **라인 48:**  \n",
    "  ```python\n",
    "  ])\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - 메시지 리스트를 닫아서, `map_doc_prompt` 템플릿 구성이 완료됨을 나타냅니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 문서 관련 추출 체인 구성 (라인 50)\n",
    "\n",
    "- **라인 50:**  \n",
    "  ```python\n",
    "  map_doc_chain = map_doc_prompt | llm\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - 위에서 생성한 `map_doc_prompt` 템플릿과 LLM 인스턴스(`llm`)를 파이프(`|`) 연산자로 연결합니다.\n",
    "    - **처리 과정:**  \n",
    "      1. 입력받은 `context`와 `question`을 템플릿에 채워서 메시지를 구성합니다.\n",
    "      2. 그 메시지를 LLM에 전달하여, 각 문서 조각에서 질문과 관련된 텍스트를 추출하도록 요청합니다.\n",
    "    - **출력:**  \n",
    "      - 각 호출 시, LLM이 반환한 응답 객체가 생성됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 여러 문서에 대한 매핑 함수 정의 (라인 52 ~ 63)\n",
    "\n",
    "- **라인 52:**  \n",
    "  ```python\n",
    "  def map_docs(inputs):\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - `map_docs`라는 함수를 정의합니다.  \n",
    "    - 이 함수는 이후 여러 문서를 받아서 각 문서에 대해 `map_doc_chain`을 호출하는 역할을 합니다.\n",
    "\n",
    "- **라인 53:**  \n",
    "  ```python\n",
    "      documents = inputs['documents']\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - `inputs` 딕셔너리에서 `documents` 키에 해당하는 값을 추출합니다.\n",
    "    - **의미:**  \n",
    "      - 이 값은 여러 문서(또는 문서 조각)들의 리스트여야 합니다.\n",
    "\n",
    "- **라인 54:**  \n",
    "  ```python\n",
    "      question = inputs['question']\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - `inputs` 딕셔너리에서 `question` 키에 해당하는 값을 추출합니다.\n",
    "    - **의미:**  \n",
    "      - 이 값은 사용자가 던진 질문(문자열)입니다.\n",
    "\n",
    "- **라인 55:**  \n",
    "  ```python\n",
    "      results = []\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - 결과를 저장할 빈 리스트를 초기화합니다.\n",
    "    - **의미:**  \n",
    "      - 각 문서에 대해 LLM의 응답(관련 텍스트)을 모아둘 공간입니다.\n",
    "\n",
    "- **라인 56:**  \n",
    "  ```python\n",
    "      for document in documents:\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - 문서 리스트에 있는 각 문서를 순회하면서 처리합니다.\n",
    "\n",
    "- **라인 57 ~ 60:**  \n",
    "  ```python\n",
    "          result = map_doc_chain.invoke({\n",
    "              \"context\": document.page_content,\n",
    "              \"question\": question\n",
    "          }).content\n",
    "  ```  \n",
    "  - **세부 설명:**  \n",
    "    - **라인 57:**  \n",
    "      - `map_doc_chain.invoke({...})`를 호출하여, 각 문서에 대해 LLM에게 질문과 문서의 내용을 전달합니다.\n",
    "    - **라인 58:**  \n",
    "      - `\"context\": document.page_content`  \n",
    "        - 각 문서 객체의 `page_content` 속성을 사용하여, 실제 텍스트 내용을 `context` 자리에 넣습니다.\n",
    "    - **라인 59:**  \n",
    "      - `\"question\": question`  \n",
    "        - 앞서 추출한 질문을 `question` 자리에 넣습니다.\n",
    "    - **라인 60:**  \n",
    "      - `.content`를 통해 LLM의 응답에서 실제 텍스트(문자열)를 추출합니다.\n",
    "    - **의미:**  \n",
    "      - 각 문서에 대해, 해당 문서 내용 중 질문과 관련된 부분을 LLM이 찾아 반환하도록 합니다.\n",
    "\n",
    "- **라인 61:**  \n",
    "  ```python\n",
    "          results.append(result)\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - LLM으로부터 받은 결과(관련 텍스트)를 `results` 리스트에 추가합니다.\n",
    "\n",
    "- **라인 62:**  \n",
    "  ```python\n",
    "      results = \"\\n\\n\".join(results)\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - 리스트에 저장된 각 결과들을 두 줄의 개행 문자(\"\\n\\n\")로 합쳐 하나의 큰 문자열로 만듭니다.\n",
    "    - **의미:**  \n",
    "      - 여러 문서에서 추출된 결과를 하나의 문맥(context)으로 통합합니다.\n",
    "\n",
    "- **라인 63:**  \n",
    "  ```python\n",
    "      return results\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - 통합된 결과 문자열을 반환합니다.\n",
    "    - **의미:**  \n",
    "      - 이 함수의 최종 출력은 모든 문서에서 추출된 관련 텍스트가 결합된 하나의 문자열입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 매핑 체인 구성 (라인 65)\n",
    "\n",
    "- **라인 65:**  \n",
    "  ```python\n",
    "  map_chain = {\"documents\": retriever, \"question\": RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - 새로운 체인(`map_chain`)을 구성합니다.\n",
    "  - **세부 설명:**  \n",
    "    - **입력 구성:**  \n",
    "      - 딕셔너리 `{ \"documents\": retriever, \"question\": RunnablePassthrough() }`  \n",
    "        - `\"documents\": retriever`  \n",
    "          - `retriever`는 벡터 스토어로부터 관련 문서들을 검색하는 역할을 합니다.\n",
    "          - 이 체인은 최종 질문을 받아 관련 문서를 찾아서 반환합니다.\n",
    "        - `\"question\": RunnablePassthrough()`  \n",
    "          - `RunnablePassthrough`는 입력된 질문을 그대로 통과시킵니다.\n",
    "    - **파이프 연산자(`|`)로 연결:**  \n",
    "      - 이후 `RunnableLambda(map_docs)`와 연결되어, 위 딕셔너리의 데이터를 `map_docs` 함수에 전달합니다.\n",
    "    - **최종 처리:**  \n",
    "      - `map_chain`은 주어진 질문을 이용하여 retriever에서 문서를 검색하고, 각 문서에 대해 `map_docs` 함수를 실행하여 관련 텍스트들을 추출 및 통합합니다.\n",
    "    - **출력:**  \n",
    "      - 통합된 관련 텍스트(문자열)가 반환됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 최종 답변 생성을 위한 프롬프트 생성 (라인 67 ~ 79)\n",
    "\n",
    "- **라인 67:**  \n",
    "  ```python\n",
    "  final_prompt = ChatPromptTemplate.from_messages([\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - 최종 답변을 생성하기 위한 또 다른 프롬프트 템플릿을 생성합니다.\n",
    "    - 이 템플릿은 이전 단계에서 추출된 텍스트(문맥)와 질문을 사용해 LLM이 최종 답변을 생성하도록 안내합니다.\n",
    "\n",
    "- **라인 68 ~ 77:**  \n",
    "  ```python\n",
    "      (\n",
    "          \"system\", \n",
    "          \"\"\"\n",
    "          Given the following extracted parts of a long docement and a question, create a final anwer.\n",
    "          If you don't know the answer, just say that you don't know.\n",
    "          Don't try to make up an answer.\n",
    "          ------\n",
    "          {context}\n",
    "          \"\"\"\n",
    "      ),\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - `\"system\"` 메시지로 LLM에게 최종 답변 생성 방법에 대한 지시사항을 제공합니다.\n",
    "  - **세부 내용:**  \n",
    "    - **지시사항:**  \n",
    "      - “Given the following extracted parts of a long docement and a question, create a final anwer.”  \n",
    "        → 추출된 문맥과 질문을 바탕으로 최종 답변을 만들어내라는 명령입니다.\n",
    "      - “If you don't know the answer, just say that you don't know.”  \n",
    "        → 답을 모를 경우 모른다고 명시하도록 합니다.\n",
    "      - “Don't try to make up an answer.”  \n",
    "        → 허구의 답변을 만들지 말라는 경고입니다.\n",
    "      - “------” 이후 `{context}` 플레이스홀더가 있어, 추출된 문맥이 이 자리에 삽입됩니다.\n",
    "    \n",
    "- **라인 78:**  \n",
    "  ```python\n",
    "      (\"human\", \"{question}\")\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - `\"human\"` 메시지로, 사용자 질문을 `{question}` 플레이스홀더에 넣습니다.\n",
    "\n",
    "- **라인 79:**  \n",
    "  ```python\n",
    "  ])\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - 프롬프트 템플릿 구성을 마칩니다.\n",
    "    - `final_prompt`는 이후 최종 답변 생성을 위한 지침과 사용자 질문을 포함하는 템플릿이 됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 최종 체인 구성 및 실행 (라인 81 ~ 83)\n",
    "\n",
    "- **라인 81:**  \n",
    "  ```python\n",
    "  chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - 최종적인 체인을 구성합니다.\n",
    "  - **세부 설명:**  \n",
    "    - **입력 구성:**  \n",
    "      - 딕셔너리 `{ \"context\": map_chain, \"question\": RunnablePassthrough() }`  \n",
    "        - `\"context\": map_chain`  \n",
    "          - 이전 단계에서 구성한 `map_chain`이 최종 프롬프트의 `{context}`에 들어갈 데이터를 생성합니다.  \n",
    "          - 이 과정에서 retriever를 통해 관련 문서를 검색하고, 각 문서에서 질문과 관련된 텍스트를 추출해 통합합니다.\n",
    "        - `\"question\": RunnablePassthrough()`  \n",
    "          - 사용자의 질문을 그대로 다음 단계로 전달합니다.\n",
    "    - **파이프 연산자(`|`)의 연결 순서:**  \n",
    "      1. **첫번째 연결:**  \n",
    "         - 입력 딕셔너리를 `final_prompt`에 전달하여, 템플릿에 `{context}`와 `{question}`을 채워넣습니다.\n",
    "      2. **두번째 연결:**  \n",
    "         - 완성된 프롬프트를 LLM(`llm`)에 전달하여 최종 답변을 생성하도록 합니다.\n",
    "    - **최종 출력:**  \n",
    "      - LLM이 생성한 최종 답변(텍스트)이 체인의 결과로 반환됩니다.\n",
    "\n",
    "- **라인 83:**  \n",
    "  ```python\n",
    "  chain.invoke(\"What is the name of Oldwyn’s mechanical doll?\")\n",
    "  ```  \n",
    "  - **역할:**  \n",
    "    - 최종 체인을 실행(호출)합니다.\n",
    "  - **세부 처리 과정:**  \n",
    "    1. **입력:**  \n",
    "       - 문자열 `\"What is the name of Oldwyn’s mechanical doll?\"`가 전달됩니다.\n",
    "    2. **처리:**  \n",
    "       - 이 질문은 `RunnablePassthrough()`를 통해 그대로 전달되며, retriever를 통해 관련 문서들이 검색됩니다.\n",
    "       - `map_chain`이 각 문서에서 관련 텍스트를 추출해 하나의 문맥으로 통합합니다.\n",
    "       - 이 통합된 문맥과 원래 질문이 `final_prompt`에 채워져 최종 프롬프트가 생성되고, LLM이 이를 바탕으로 답변을 만듭니다.\n",
    "    3. **출력:**  \n",
    "       - LLM이 최종적으로 생성한 답변이 반환됩니다.\n",
    "  - **의미:**  \n",
    "    - 전체 체인 과정을 통해, 주어진 질문에 대해 관련 문서에서 필요한 정보를 추출하고 최종적으로 정확한 답변을 생성하게 됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "이와 같이, 37번째 줄부터 84번째 줄까지의 코드는 다음과 같은 큰 흐름을 따릅니다:\n",
    "\n",
    "1. **문서의 관련 텍스트 추출:**  \n",
    "   - 각 문서 조각에 대해 LLM이 질문과 관련된 부분을 찾아 반환하도록 함.\n",
    "2. **추출된 텍스트의 통합:**  \n",
    "   - 여러 문서에서 나온 결과를 하나의 큰 문맥으로 합침.\n",
    "3. **최종 답변 생성:**  \n",
    "   - 통합된 문맥과 질문을 바탕으로 LLM이 최종 답변을 생성함.\n",
    "\n",
    "이로써, 질문 `\"What is the name of Oldwyn’s mechanical doll?\"`에 대해 체계적으로 관련 정보를 검색하고, 추출한 후 최종 답변을 만들어내는 전체 파이프라인이 완성됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
