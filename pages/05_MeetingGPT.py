import streamlit as st
import subprocess
from pydub import AudioSegment
import math
import glob
import openai
import os
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import StrOutputParser
from langchain.vectorstores.faiss import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.storage import LocalFileStore
from langchain.embeddings import CacheBackedEmbeddings

st.set_page_config(
    page_icon="ğŸ¤–",
    page_title="MeetingGPT",
)

llm = ChatOpenAI(
    temperature=0.1,
    model="gpt-4o",
)

has_transcript = False

# os.path.exists("./.cache/podcast.txt")

client = openai.OpenAI()

splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=800,
    chunk_overlap=200,
)


@st.cache_resource()
def embed_file(file_path):
    file_name = os.path.basename(file_path)
    loader = TextLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    embeddings = OpenAIEmbeddings()
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file_name}")
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
        embeddings,
        cache_dir,
    )
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever


@st.cache_resource()
def extract_audio_from_video(video_path):
    if has_transcript:
        return
    audio_path = video_path.replace("mp4", "mp3")
    command = [
        "ffmpeg",
        "-y",
        "-i",
        video_path,
        "-vn",
        audio_path,
    ]
    # ì´ë ‡ê²Œ ë°°ì—´ë¡œ í˜•íƒœë¡œ ì „ë‹¬í•˜ëŠ” ê²ƒì€ ë³´ì•ˆë•Œë¬¸ì´ë‹¤. ë˜í•œ íŠ¹ìˆ˜ ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆì„ë•Œë„ ë” í¸ë¦¬í•˜ê¸° ë•Œë¬¸ì—
    # ë°°ì—´ë°©ì‹ì´ ê¶Œì¥ëœë‹¤.
    subprocess.run(command)


@st.cache_resource()
def cut_audio_in_chunks(audio_path, chunk_size, chunks_folder):
    if has_transcript:
        return
    track = AudioSegment.from_mp3(audio_path)
    chunk_len = chunk_size * 60 * 1000  # 5ë¶„ì„ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
    chunks = math.ceil(len(track) / chunk_len)  # ceilì€ ì˜¬ë¦¼ í• ë•Œ ì“°ëŠ” í•¨ìˆ˜

    for i in range(chunks):
        start_time = i * chunk_len
        end_time = (i + 1) * chunk_len

        if i < chunks - 1:
            splited_track = track[start_time:end_time]
            splited_track.export(f"{chunks_folder}/chunk_{i}.mp3", format="mp3")
        else:
            splited_track = track[start_time:]
            splited_track.export(f"{chunks_folder}/chunk_{i}.mp3", format="mp3")


@st.cache_resource()
def transcribe_chunks(chunk_folder, destination):
    if has_transcript:
        return
    files = glob.glob(f"{chunk_folder}/*.mp3")
    files.sort()  # globìœ¼ë¡œ íŒŒì¼ì„ ê°€ì ¸ì˜¬ë•Œ íŒŒì¼ì„ ê°€ì ¸ì˜¤ëŠ” ìˆœì„œëŠ” ë’¤ì£½ë°•ì£½ì´ë‹¤.
    # ë”°ë¼ì„œ íŒŒì¼ì„ ê°€ì ¸ì™€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ê³  ê·¸ ë¦¬ìŠ¤íŠ¸ë¥¼ sortë¡œ ì •ë ¬í•´ì¤˜ì•¼ í•œë‹¤.
    for file in files:
        with open(file, "rb") as audio_file, open(destination, "a") as text_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
            )
            text_file.write(transcript.text)


st.markdown(
    """
    # MeetingGPT
            
    ë¯¸íŒ…GPTì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•˜ë©°, ë™ì˜ìƒì„ ì—…ë¡œë“œí•˜ë©´ ëŒ€ë³¸ê³¼ ìš”ì•½, ì±„íŒ… ë´‡ì„ í†µí•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ì‚¬ì´ë“œë°”ì—ì„œ ë™ì˜ìƒ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”.

"""
)

with st.sidebar:
    video = st.file_uploader(
        "Videoë¥¼ ì—…ë¡œë“œ í•˜ì„¸ìš”.",
        type=["mp4", "avi", "mkv", "mov"],
    )

if video:
    chunks_folder = "./.cache/chunks"

    temp = os.path.exists(f"./.cache/{video.name.replace('mp4', 'txt')}")

    if temp:
        has_transcript = True

    with st.status("ë¹„ë””ì˜¤ íŒŒì¼ ì½ëŠ”ì¤‘...") as status:
        video_content = video.read()
        video_path = f"./.cache/{video.name}"
        transcript_path = video_path.replace("mp4", "txt")
        with open(video_path, "wb") as f:
            f.write(video_content)
        status.update(label="ì˜¤ë””ì˜¤ ì¶”ì¶œì¤‘...")
        extract_audio_from_video(video_path)
        status.update(label="ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„í• ì¤‘...")
        audio_path = video_path.replace("mp4", "mp3")
        cut_audio_in_chunks(audio_path, 10, chunks_folder)
        status.update(label="ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œì¤‘...")
        transcribe_chunks(chunks_folder, transcript_path)

    transcript_tab, summary_tab, qa_tab = st.tabs(
        [
            "ìŠ¤í¬ë¦½íŠ¸",
            "ìš”ì•½",
            "ì±„íŒ…",
        ]
    )

    with transcript_tab:
        with open(transcript_path, "r") as file:
            st.write(file.read())

    with summary_tab:
        start = st.button("ìš”ì•½ë³¸ ë¶ˆëŸ¬ì˜¤ê¸°")

        if start:

            loader = TextLoader(transcript_path)

            docs = loader.load_and_split(text_splitter=splitter)
            first_summary_prompt = ChatPromptTemplate.from_template(
                """
                Write a concise summary of the following in Korean so that even an elementary school student can easily understand it.:
                "{text}"

                CONCISE SUMMARY
                """
            )

            first_summary_chain = first_summary_prompt | llm | StrOutputParser()

            summary = first_summary_chain.invoke({"text": docs[0].page_content})

            refine_prompt = ChatPromptTemplate.from_template(
                """
                Your job is to produce a final summary.
                We have provided an existing summary up to a certain point: {existing_summary}
                We have the opportunity to refine the existing summary (only if needed) with some more context below.
                -----------
                {context}
                -----------
                Given the new context, refine the original summary.
                If the context isn't useful, RETURN the original summary.
                If you write the refined summary, please write in Korean, making it simple enough for an elementary school student to understand easily.
                """
            )

            refine_chain = refine_prompt | llm | StrOutputParser()

            with st.status("ìš”ì•½ ì§„í–‰ì¤‘...") as status:
                for i, doc in enumerate(docs[1:]):
                    status.update(label=f"ë¬¸ì„œ ìš”ì•½ì¤‘: {i+1}/{len(docs)-1}")
                    summary = refine_chain.invoke(
                        {
                            "existing_summary": summary,
                            "context": doc.page_content,
                        }
                    )
            st.write(summary)

    with qa_tab:
        retriever = embed_file(transcript_path)

        docs = retriever.invoke("ì–´ë–¤ ì–˜ê¸°ë¥¼ í•˜ê³  ìˆë‚˜ìš”? í•œë¬¸ì¥ìœ¼ë¡œ ë§í•´ì£¼ì„¸ìš”.")

        st.write(docs)
