from typing import Dict, List
from uuid import UUID
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st

st.set_page_config(
    page_icon="ğŸ¤–",
    page_title="DocumentGPT",
)


class ChatCallbackHandler(BaseCallbackHandler):

    message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)


llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(),
    ],
)


@st.cache_resource(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    embeddings = OpenAIEmbeddings()
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
        embeddings,
        cache_dir,
    )
    vectorstore = Chroma.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever
    # @st.cache_resourceë¼ëŠ” ë°ì½”ë ˆì´í„°ë¥¼ í†µí•´ì„œ, Streamlitì€ ì—¬ê¸°ì— ì–´ë–¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸ë¶€í„° í• ê²ƒì´ë‹¤.
    # ê·¸ë¦¬ê³  íŒŒì¼ì´ ë™ì¼í•˜ë‹¤ë©´, Streamlitì€ ì´ í•¨ìˆ˜ë¥¼ ì¬ì‹¤í–‰ì‹œí‚¤ì§€ ì•Šì„ ê²ƒì´ë‹¤.
    # ë‹¤ì‹œ ë§í•´ì„œ ì´ embed_fileì´ë¼ëŠ” í•¨ìˆ˜ëŠ” ì²˜ìŒì—ëŠ” ì‹¤í–‰ ë  ê²ƒì´ë‹¤.
    # ë‘ë²ˆì§¸ì—ëŠ” íŒŒì¼ì´ ë³€í•˜ì§€ ì•Šì•˜ì„ ë•Œ, ì¦‰ ìœ ì €ê°€ ë‹¤ë¥¸ íŒŒì¼ì„ ì˜¬ë¦¬ì§€ ì•Šì•˜ì„ ë•Œ í•¨ìˆ˜ê°€ ì¬ì‹¤í–‰ë˜ëŠ” ëŒ€ì‹ ì—,
    # Streamlitì€ í•¨ìˆ˜ ì‹¤í–‰ì„ ê±´ë„ˆ ë›¸ ê²ƒì´ê³ , ê·¸ë¦¬ê³  ì´ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ëŠ” ëŒ€ì‹ ì— ê¸°ì¡´ì— ë°˜í™˜í–ˆë˜ ê°’ì„ ë‹¤ì‹œ ë°˜í™˜í•  ê²ƒì´ë‹¤.


def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})


def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)


def paint_history():
    for message in st.session_state["messages"]:
        send_message(message["message"], message["role"], save=False)


def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)
    # ë°©ë²• 2: ì¼ë°˜ì ì¸ for ë°˜ë³µë¬¸(ìœ„ì˜ ì½”ë“œì™€ ì™„ì „ ê°™ì€ ì½”ë“œ)
    # result = []
    # for document in docs:
    #     result.append(document.page_content)


prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            Answer the question using ONLY the following context.
            If you don't know the answer just say you don't know.
            DON'T make anything up.
            And if the user asks a question in Korean, answer in Korean, and if the user asks a question in English, answer in English.
     
            Context: {context}
            """,
        ),
        ("human", "{question}"),
    ]
)

st.title("DocumentGPT")

st.markdown(
    """
    ì•ˆë…•í•˜ì„¸ìš”!

    ì±—ë´‡ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ì— ëŒ€í•´ ì¸ê³µì§€ëŠ¥ì—ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”.

    íŒŒì¼ì„ ì‚¬ì´ë“œë°”ì— ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.
    """
)

with st.sidebar:
    file = st.file_uploader(
        "txt, pdf, docx íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì„¸ìš”.",
        type=["pdf", "txt", "docx"],
    )

if file:
    retriever = embed_file(file)
    send_message("íŒŒì¼ì„ ëª¨ë‘ ì½ì—ˆìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?", "ai", save=False)
    paint_history()
    message = st.chat_input("íŒŒì¼ì— ëŒ€í•´ ë¬´ì—‡ì´ë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”.")
    if message:
        send_message(message, "human")
        chain = (
            {
                "context": retriever | RunnableLambda(format_docs),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        with st.chat_message("ai"):
            chain.invoke(message)
else:
    st.session_state["messages"] = []
